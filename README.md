# optimization_GD
This is a notebook compilation where I test some gradient descent methods for optimization. 

Here are some variations to study: 
- adding penalties/regularisation to the loss function 
- compare GD Stochastic and GD Batch 
- compare Coordinate GD and Coordinate SGD
- compare Momentum and GD

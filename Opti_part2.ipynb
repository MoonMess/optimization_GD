{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(W, x):\n",
    "    value = np.dot(W, x)\n",
    "    \n",
    "    def vjp(u): # Same shape as ‘value’\n",
    "        return np.outer(u, x), W.T.dot(u)\n",
    "\n",
    "    return value, vjp\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: implement the relu function and its VJP in the format above. Using the finite difference equation (slide 13), make sure that the VJP is correct numerically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical check : [[-1.39777967e-10 -2.79555934e-10 -4.19333901e-10]]\n"
     ]
    }
   ],
   "source": [
    "#let's define epsilon\n",
    "epsilon = 1e-6\n",
    "x = np.array([4,5,6])\n",
    "u = np.array([1,2,3])\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    value = np.maximum(0,x)\n",
    "    \n",
    "    def vjp(u):\n",
    "        #computing relu derivation\n",
    "        vjp_wrt_x = u*np.greater(x,0)\n",
    "        return vjp_wrt_x,  # The comma is important! to return a tuple of size 1 and not a scalar\n",
    "\n",
    "    return value, vjp\n",
    "\n",
    "#---------------------------------------------------\n",
    "\n",
    "def finite_dif(x):\n",
    "    dif = (relu(x+epsilon)[0]-relu(x)[0])/epsilon\n",
    "    return dif\n",
    "\n",
    "\n",
    "value, vjp = relu(x)\n",
    "dif = finite_dif(x)\n",
    "\n",
    "print(\"Numerical check :\", vjp(u) - u.T*dif )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the difference between vjp and finite difference equation is in the order of $10^{-10}$, negligeable difference. We can considere it as correct numerically."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: reusing dot and relu, implement a 2-layer MLP with a relu activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp2(x, W1, W2):\n",
    "    value_W1,vjp_W1_x = dot(W1,x)       #return value,vjp\n",
    "    value_relu,vjp_relu = relu(value_W1) #return value,vjp\n",
    "    value,vjp_W2_relu = dot(W2,value_relu) #return value,vjp\n",
    "    def vjp(u):\n",
    "        vjp_wrt_W2,vjp_wrt_relu = vjp_W2_relu(u)\n",
    "        val_vjp_relu, = vjp_relu(vjp_wrt_relu)\n",
    "        vjp_wrt_W1, vjp_wrt_x  = vjp_W1_x(val_vjp_relu)\n",
    "        return vjp_wrt_x, vjp_wrt_W1, vjp_wrt_W2 \n",
    "\n",
    "    return value, vjp #output & vjp\n",
    "\n",
    "#TEST#\n",
    "#2 layers & 2 inputs\n",
    "# W1 = np.array([[0.5,0.45],[0.40,0.35]])\n",
    "# W2 = np.array([[0.30,0.25],[0.20,0.15]])\n",
    "\n",
    "# x = np.array([0.4,0.6])\n",
    "\n",
    "#2 layers & 2 inputs\n",
    "x = np.array([1,2])\n",
    "W1 = np.array([[6,8],[3,5]])\n",
    "W2 = np.array([[1,8],[6,0]])\n",
    "\n",
    "\n",
    "\n",
    "output, vjpmlp2 = mlp2(x, W1, W2)\n",
    "\n",
    "#u = np.array([1,3])\n",
    "#print(vjpmlp2(u))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the 2-layer MLP, with a 2 dimension input and output. (We will compute a final check later, after loss implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([126, 132])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: implement the squared loss VJP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_pred, y):\n",
    "    residual = y_pred - y\n",
    "    \n",
    "    def vjp(u):\n",
    "        vjp_y_pred = u*(residual)\n",
    "        vjp_y =u*(-residual)\n",
    "        return vjp_y_pred, vjp_y\n",
    "\n",
    "    value = 0.5 * np.sum(residual ** 2)\n",
    "    # The code requires every output to be an array.\n",
    "    return np.array([value]), vjp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 : implement the loss by composing mlp2 and squared_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, y, W1, W2):\n",
    "  output_mlp2, vjp_mlp2 = mlp2(x, W1, W2)\n",
    "  value, vjp_sqloss = squared_loss(output_mlp2, y)\n",
    "  def vjp(u):\n",
    "    vjp_wrt_output, vjp_wrt_y = vjp_sqloss(u)\n",
    "    vjp_wrt_x, vjp_wrt_W1, vjp_wrt_W2 = vjp_mlp2(vjp_wrt_output)\n",
    "    return  vjp_wrt_x, vjp_wrt_y, vjp_wrt_W1, vjp_wrt_W2\n",
    "\n",
    "  return value, vjp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([126, 132])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp2(x, W1, W2)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now check the MLP2 with square_loss numerically : (check question 2 and 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical check on x :  [0. 0.]\n",
      "test OK\n",
      "---------------------------------------\n",
      "Numerical check :  [-1.e-10 -1.e-10]\n",
      "---------------------------------------\n",
      "Numerical check :  [[-0. -0.]\n",
      " [-0. -0.]]\n",
      "test OK\n",
      "---------------------------------------\n",
      "Numerical check :  [[0. 0.]\n",
      " [0. 0.]]\n",
      "test OK\n"
     ]
    }
   ],
   "source": [
    "def finite_dif(x,y,W1,W2,epsilon):\n",
    "    x_grad = x.copy()\n",
    "    y_grad = y.copy()\n",
    "    W1_grad = W1.copy()\n",
    "    W2_grad = W2.copy()\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "            x_perturb=x.copy()\n",
    "            x_perturb[i]+=epsilon\n",
    "            x_grad[i]=((loss(x_perturb,y,W1,W2)[0]-loss(x,y,W1,W2)[0])/epsilon)\n",
    "    \n",
    "    for i in range(y.shape[0]):\n",
    "            y_perturb=y.copy()\n",
    "            y_perturb[i]+=epsilon\n",
    "            y_grad[i]=((loss(x,y_perturb,W1,W2)[0]-loss(x,y,W1,W2)[0])/epsilon)\n",
    "\n",
    "    for i in range(W1.shape[0]):\n",
    "        for j in range(W1.shape[1]):\n",
    "            W1_perturb=W1.copy()\n",
    "            W1_perturb[i,j]+=epsilon\n",
    "            W1_grad[i,j]=((loss(x,y,W1_perturb,W2)[0]-loss(x,y,W1,W2)[0])/epsilon)\n",
    "\n",
    "    for i in range(W2.shape[0]):\n",
    "        for j in range(W2.shape[1]):\n",
    "            W2_perturb=W2.copy()\n",
    "            W2_perturb[i,j]+=epsilon\n",
    "            W2_grad[i,j]=((loss(x,y,W1,W2_perturb)[0]-loss(x,y,W1,W2)[0])/epsilon)\n",
    "            \n",
    "    return(x_grad,y_grad,W1_grad,W2_grad)\n",
    "\n",
    "\n",
    "#let's define epsilon\n",
    "# epsilon = 1e-6\n",
    "# x = np.array([1.,2.])\n",
    "# #u = np.array([[1.,2.]])\n",
    "# y = np.array([1.,2.])\n",
    "# W1 = np.array([[0.3,0.35],[0.4,0.45]])\n",
    "# W2 = np.array([[0.5,0.55],[0.6,0.65]])\n",
    "u = np.array([1.])\n",
    "\n",
    "x  = np.random.uniform(-10, 10, size=2)\n",
    "#W  = np.random.uniform(size=(6,9))\n",
    "W1 = np.random.uniform(size=(2,2))\n",
    "W2 = np.random.uniform(size=(2,2))\n",
    "#y_pred = np.random.uniform(size=1)\n",
    "y   = np.random.randint(2, size=1).astype(float)\n",
    "epsilon   = 1e-10\n",
    "\n",
    "condition4grad2beOK = 1e-5\n",
    "\n",
    "value, vjp = loss(x,y,W1,W2)\n",
    "\n",
    "grad = finite_dif(x,y,W1,W2,epsilon)\n",
    "\n",
    "\n",
    "# x_dif = vjp(u)[0] - u.T*grad[0]\n",
    "# print(\"Numerical check on x : \", x_dif)\n",
    "# if (np.all(x_dif) <= condition4grad2beOK) : print(\"test OK\")\n",
    "# print('---------------------------------------')\n",
    "# y_dif = vjp(u)[1] - u.T*grad[1]\n",
    "# print(\"Numerical check : \", y_dif ) \n",
    "# if (np.all(y_dif) <= condition4grad2beOK) : print(\"test OK\")\n",
    "# print('---------------------------------------')\n",
    "# W1_dif = vjp(u)[2] - u.T*grad[2]\n",
    "# print(\"Numerical check : \", W1_dif )\n",
    "# if (np.all(W1_dif) <= condition4grad2beOK) : print(\"test OK\")\n",
    "# print('---------------------------------------')\n",
    "# W2_dif = vjp(u)[3] - u.T*grad[3]\n",
    "# print(\"Numerical check : \", W2_dif)\n",
    "# if (np.all(W2_dif) <= condition4grad2beOK) : print(\"test OK\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test is ok, it seems to work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testVJP(func, *args):\n",
    "    \n",
    "    value, vjp = func(*args)\n",
    "    u = np.random.random(value.shape)\n",
    "    e   = 1e-10\n",
    "\n",
    "    for k, [arg, vjp] in enumerate(zip(args, vjp(u))):\n",
    "        \n",
    "        shape = arg.shape\n",
    "        num = np.zeros_like(arg)\n",
    "        \n",
    "        if len(shape)>1:\n",
    "            \n",
    "            for i in range(shape[0]):\n",
    "                for j in range(shape[1]):\n",
    "                    arg_e = np.copy(arg)\n",
    "                    args_e = list(args)\n",
    "                    arg_e[i, j] += e\n",
    "                    args_e[k] = arg_e\n",
    "                    num[i, j] = (func(*args_e)[0] - value)/e \n",
    "                    \n",
    "        else:\n",
    "            \n",
    "            for i in range(shape[0]):\n",
    "                    arg_e = np.copy(arg)\n",
    "                    args_e = list(args)\n",
    "                    arg_e[i] += e\n",
    "                    args_e[k] = arg_e\n",
    "                    num[i] = (func(*args_e)[0] - value)/e                    \n",
    "                    \n",
    "        num = np.array(num)\n",
    "        print(f\"VJP_var_{k} OK : \", np.all(np.abs(vjp - (u * num)) < condition4grad2beOK),\n",
    "              f\"(max diff : {np.max(np.abs(vjp - (u * num)))})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = np.random.random(value.shape)\n",
    "type(u[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VJP_var_0 OK :  True (max diff : 0.0)\n",
      "VJP_var_1 OK :  True (max diff : 5.524495245560156e-11)\n",
      "VJP_var_2 OK :  True (max diff : 0.0)\n",
      "VJP_var_3 OK :  True (max diff : 0.0)\n"
     ]
    }
   ],
   "source": [
    "condition4grad2beOK = 1e-5\n",
    "testVJP(loss, x,y,W1,W2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: implement an MLP with an arbitrary number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W is the list of [W1,...,WN]\n",
    "def mlpN(x, W):\n",
    "    value_vjp_W = np.array([dot(W[0],x)],dtype=object) #initialisation value_W1,vjp_W1_relu\n",
    "    vjps_relu = np.array([],dtype=object)\n",
    "    N = W.shape[0] #nbr of layers\n",
    "    for i in range(1,N): \n",
    "        #going through layers 2 until N \n",
    "        #print(i)\n",
    "        value_relu,vjp_relu = relu(value_vjp_W[i-1][0])\n",
    "        #value_vjp_relu = np.append(value_vjp_relu,[relu(value_vjp_W[i-1][0])],axis=0)    #value_relu,vjp_relu\n",
    "        vjps_relu= np.append(vjps_relu,vjp_relu)\n",
    "        value_vjp_W = np.append(value_vjp_W,[dot(W[i],value_relu)],axis=0)    #value_Wi,vjp_Wi_relu \n",
    "        \n",
    "    def vjp(u):\n",
    "        #init for layers N\n",
    "        vjp_wrt_WN,vjp_wrt_relu = value_vjp_W[-1][1](u) #vjp_WN_relu(u)\n",
    "        vjp_wrt_relu=np.array([vjp_wrt_relu]) \n",
    "        vjp_wrt_W=np.array([vjp_wrt_WN])\n",
    "\n",
    "        for i in range(N-2,0,-1):\n",
    "            #iteration on layers from N-1 to 2  \n",
    "            val_vjp_relu, = vjps_relu[i][1](vjp_wrt_relu[-1]) #vjp_relu(vjp_wrt_relu)\n",
    "            vjp_wrt_Wi,vjp_wrt_relu = value_vjp_W[i][1](val_vjp_relu)\n",
    "\n",
    "            vjp_wrt_relu = np.append(vjp_wrt_relu,vjp_wrt_relu,axis=0)\n",
    "            vjp_wrt = np.append(vjp_wrt_W,vjp_wrt_Wi,axis=0)\n",
    "\n",
    "        #iteration on layers 1\n",
    "        print(vjps_relu)\n",
    "        val_vjp_relu, = vjps_relu[0][1](vjp_wrt_relu[-1])\n",
    "        vjp_wrt_W1, vjp_wrt_x  = value_vjp_W[0][1](val_vjp_relu)\n",
    "        vjp_wrt_W = np.append(vjp_wrt_W,vjp_wrt_W1,axis=0)\n",
    "        vjp_wrt_W = np.append(vjp_wrt_W,vjp_wrt_x,axis=0)\n",
    "\n",
    "        return np.flip(vjp_wrt_W)\n",
    "        \n",
    "    value = value_vjp_W[-1][0]\n",
    "    return value, vjp #output & vjp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp2(x, W1, W2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0, 0.0], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpN(x, np.array([W1, W2],dtype=object))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the same result for both, the network weems to work."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: implement SGD to train your MLP on a dataset of your choice. Study the impact of depth (number of layers) and width (number of hidden units)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset : https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math\n",
    "from numpy.linalg import norm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/winequality-red.csv', sep=',')\n",
    "data = data.drop_duplicates()\n",
    "# copy the data\n",
    "data_scaled = data.copy()\n",
    "mean_data = data_scaled.mean()\n",
    "std_data =  data_scaled.std()\n",
    "\n",
    "# apply normalization techniques\n",
    "for column in data_scaled.columns:\n",
    "    if column!='quality':\n",
    "        data_scaled[column] = (data_scaled[column] - mean_data[column]) / std_data[column]\n",
    "    else :\n",
    "        data_scaled[column] = (data_scaled[column] - mean_data[column])\n",
    "\n",
    "\n",
    "df_label = data_scaled['quality']\n",
    "df_features = data_scaled.drop(columns=['quality']) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features, df_label, test_size=0.33, random_state=42, shuffle = True)\n",
    "\n",
    "A_test = X_test.to_numpy() #training matrix\n",
    "Y_test = y_test.to_numpy() #label matrix\n",
    "A_train = X_train.to_numpy() #training matrix\n",
    "Y_train = y_train.to_numpy() #label matrix\n",
    "n, p = X_train.shape #number of feature\n",
    "\n",
    "#x0 = np.random.rand(p).T #random initial point \n",
    "x0 = np.zeros((p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_pred, y):\n",
    "    residual = y_pred - y\n",
    "    \n",
    "    def vjp(u):\n",
    "        vjp_y_pred = u*(residual)\n",
    "        vjp_y = u*(-residual)\n",
    "        return vjp_y_pred, vjp_y\n",
    "\n",
    "    value = 0.5 * np.sum(residual ** 2)\n",
    "    # The code requires every output to be an array.\n",
    "    return np.array([value]), vjp\n",
    "\n",
    "def loss(x, y, W):\n",
    "  output_mlp, vjp_mlp = mlpN(x, W)\n",
    "  value, vjp_sqloss = squared_loss(output_mlp, y)\n",
    "  def vjp(u):\n",
    "    vjp_wrt_output, vjp_wrt_y = vjp_sqloss(u)\n",
    "    vjp_wrt_var = vjp_mlp(vjp_wrt_output)\n",
    "    vjp_wrt_var.append(vjp_wrt_y)\n",
    "    return  vjp_wrt_var\n",
    "\n",
    "  return value, vjp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09082838,  4.40509777,  0.1339666 , ..., -4.48261231,\n",
       "        -4.31592649, -4.56527795],\n",
       "       [ 0.09082838,  4.40509777,  0.1339666 , ..., -4.48261231,\n",
       "        -4.31592649, -4.56527795]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(W.T,A_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int32"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(error[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype=np.float64(error)\n",
    "type(dtype[0][1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 8])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(A_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (11,) and (2,11) not aligned: 11 (dim 0) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [126], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m loss_list\u001b[39m=\u001b[39m[]\n\u001b[0;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iterations):\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m     \u001b[39m#  forward pass \u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     output, vjp \u001b[39m=\u001b[39m mlpN(W, A_train)\n\u001b[0;32m     23\u001b[0m     error \u001b[39m=\u001b[39m output \u001b[39m-\u001b[39m y_train\n\u001b[0;32m     24\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn [46], line 3\u001b[0m, in \u001b[0;36mmlpN\u001b[1;34m(x, W)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmlpN\u001b[39m(x, W):\n\u001b[1;32m----> 3\u001b[0m     value_vjp_W \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([dot(W[\u001b[39m0\u001b[39;49m],x)],dtype\u001b[39m=\u001b[39m\u001b[39mobject\u001b[39m) \u001b[39m#initialisation value_W1,vjp_W1_relu\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     vjps_relu \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([],dtype\u001b[39m=\u001b[39m\u001b[39mobject\u001b[39m)\n\u001b[0;32m      5\u001b[0m     N \u001b[39m=\u001b[39m W\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m#nbr of layers\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [14], line 2\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(W, x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdot\u001b[39m(W, x):\n\u001b[1;32m----> 2\u001b[0m     value \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(W, x)\n\u001b[0;32m      4\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mvjp\u001b[39m(u): \u001b[39m# Same shape as ‘value’\u001b[39;00m\n\u001b[0;32m      5\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mouter(u, x), W\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mdot(u)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (11,) and (2,11) not aligned: 11 (dim 0) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#W1 = np.array([[6,8],[3,5]])\n",
    "#W2 = np.array([[1,8],[6,0]])\n",
    "\n",
    "# Initialize the weight matrices for each layer\n",
    "#W = np.array([W1,W2])\n",
    "#print(len(W_list))\n",
    "W = np.ones([2,11])\n",
    "X_train = np.array([9,8])\n",
    "y_train = np.array([1,7])\n",
    "\n",
    "# Define the number of iterations and the learning rate\n",
    "num_iterations = 100\n",
    "learning_rate = 0.1\n",
    "loss_list=[]\n",
    "for i in range(num_iterations):\n",
    "\n",
    "    #  forward pass \n",
    "    output, vjp = mlpN(W, A_train)\n",
    "    error = output - y_train\n",
    "    if i % 10 == 0:\n",
    "        print(f'iteration: {i}, loss: {np.mean(error ** 2)}')\n",
    "    loss_list.append(np.mean(error ** 2))\n",
    "    \n",
    "    # gradients\n",
    "    vjp_x, vjp_W = vjp(error)\n",
    "\n",
    "    # weights update\n",
    "    for i in range(len(W)):\n",
    "        W[i] = W[i] - learning_rate * vjp_W[i]\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.rcParams['figure.dpi'] = 227\n",
    "plt.plot(range(len(loss_list)), loss_list, c= 'darkorchid', lw=1)\n",
    "plt.title('Mean square error', fontSize=14)\n",
    "plt.xlabel('Iterations', fontSize=11)\n",
    "plt.ylabel('MSE', fontSize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(x0, A, y, epochs = 50, nb=1 ,learning_rate = 0.01, stopping_threshold = 1e-6):\n",
    "    # Initializing learning rate and iterations\n",
    "    learning_rate = learning_rate #step size\n",
    "    losses = [] #to return \n",
    "    vector=x0.copy() #starting point\n",
    "    n,p = A.shape #nb of data point\n",
    "    u = np.array([1.])\n",
    "    # Estimation of optimal parameters\n",
    "    for _ in range(epochs):\n",
    "        print(_)\n",
    "        ik = [int(el) for el in np.floor(np.random.rand(nb)*n)]\n",
    "        # Calculating a new prediction\n",
    "        #prediction = A[ik,:].dot(vector)\n",
    "\n",
    "        #mean square error\n",
    "        #error = prediction - y[ik] \n",
    "        #mse = 1/(2*nb)*(np.dot(error.T,error))\n",
    "        error, vjp = loss(A[ik,:], y[ik], vector)\n",
    "        print(error)\n",
    "        losses.append(error)\n",
    "\n",
    "        #calcultating gradient\n",
    "        #grad_k = (1/nb)*A[ik,:].transpose().dot(error)\n",
    "        grad_k = vjp(u)\n",
    "        print(grad_k)\n",
    "        #gradient descent step\n",
    "        vector = vector - (learning_rate * grad_k)\n",
    "\n",
    "    return vector, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the weight matrices for each layer\n",
    "W = np.zeros([1,11,2])\n",
    "\n",
    "iteration=100\n",
    "lr = 0.1\n",
    "\n",
    "for i in range(iteration):\n",
    "    #forward pass\n",
    "    output, vjp = mlpN(A_train,W)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[23.50083438]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [105], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39m#x0 = np.zeros((p))\u001b[39;00m\n\u001b[0;32m      3\u001b[0m x0 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros([\u001b[39m1\u001b[39m,\u001b[39m11\u001b[39m,\u001b[39m1\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m final_x , loss \u001b[39m=\u001b[39m SGD(x0, A_train , Y_train)\n\u001b[0;32m      5\u001b[0m plt\u001b[39m.\u001b[39mplot(loss)\n\u001b[0;32m      8\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mMean square Error, stepsize at 0.01\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [86], line 24\u001b[0m, in \u001b[0;36mSGD\u001b[1;34m(x0, A, y, epochs, nb, learning_rate, stopping_threshold)\u001b[0m\n\u001b[0;32m     20\u001b[0m losses\u001b[39m.\u001b[39mappend(error)\n\u001b[0;32m     22\u001b[0m \u001b[39m#calcultating gradient\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m#grad_k = (1/nb)*A[ik,:].transpose().dot(error)\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m grad_k \u001b[39m=\u001b[39m vjp(u)\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(grad_k)\n\u001b[0;32m     26\u001b[0m \u001b[39m#gradient descent step\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [85], line 18\u001b[0m, in \u001b[0;36mloss.<locals>.vjp\u001b[1;34m(u)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvjp\u001b[39m(u):\n\u001b[0;32m     17\u001b[0m   vjp_wrt_output, vjp_wrt_y \u001b[39m=\u001b[39m vjp_sqloss(u)\n\u001b[1;32m---> 18\u001b[0m   vjp_wrt_var \u001b[39m=\u001b[39m vjp_mlp(vjp_wrt_output)\n\u001b[0;32m     19\u001b[0m   vjp_wrt_var\u001b[39m.\u001b[39mappend(vjp_wrt_y)\n\u001b[0;32m     20\u001b[0m   \u001b[39mreturn\u001b[39;00m  vjp_wrt_var\n",
      "Cell \u001b[1;32mIn [104], line 31\u001b[0m, in \u001b[0;36mmlpN.<locals>.vjp\u001b[1;34m(u)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39m#iteration on layers 1\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39mprint\u001b[39m(vjps_relu)\n\u001b[1;32m---> 31\u001b[0m val_vjp_relu, \u001b[39m=\u001b[39m vjps_relu[\u001b[39m0\u001b[39;49m][\u001b[39m1\u001b[39m](vjp_wrt_relu[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m     32\u001b[0m vjp_wrt_W1, vjp_wrt_x  \u001b[39m=\u001b[39m value_vjp_W[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m](val_vjp_relu)\n\u001b[0;32m     33\u001b[0m vjp_wrt_W \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(vjp_wrt_W,vjp_wrt_W1,axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "#x0 = np.zeros((p))\n",
    "x0 = np.zeros([1,11,1])\n",
    "final_x , loss = SGD(x0, A_train , Y_train)\n",
    "plt.plot(loss)\n",
    "\n",
    "\n",
    "plt.title('Mean square Error, stepsize at 0.01')\n",
    "plt.xlabel('No. of iterations')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I didn't succeed this part, I had troubles on the implementation, mainly about simension during vjp calculation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d27934bfc71f2157abccadd16d0d15d05f8e5134522e440e457219f263f6fb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
